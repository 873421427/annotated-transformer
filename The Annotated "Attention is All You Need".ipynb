{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"aiayn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When teaching, I emphasize implementation as a way to understand recent developments in ML. This post is an attempt to keep myself honest along this goal. The recent [\"Attention is All You Need\"]\n",
    "(https://arxiv.org/abs/1706.03762) paper from NIPS 2017 has been instantly impactful paper as a new method for machine translation and potentiall NLP generally. The paper is very clearly written, but the conventional wisdom has been that it is quite difficult to implement correctly. \n",
    ">\n",
    "> In this post I follow the paper through from start to finish and try to implement each component in code. \n",
    "(I have done some minor reordering and skipping from the original paper). This document itself is a working notebook, and should be a completely usable and efficient implementation. To follow along you will first need to install [PyTorch](http://pytorch.org/) and [torchtext](https://github.com/pytorch/text). The complete code is available on [github](https://github.com/harvardnlp/annotated-transformer).\n",
    ">- Alexander \"Sasha\" Rush ([@harvardnlp](https://twitter.com/harvardnlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Table of Contents                               \n",
    "{:toc}      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
    "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
    "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
    "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
    "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
    "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
    "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
    "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
    "described in section 3.2.\n",
    "\n",
    "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
    "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
    "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
    "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
    "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned\n",
    "recurrence and have been shown to perform well on simple-language question answering and\n",
    "language modeling tasks [34].\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
    "entirely on self-attention to compute representations of its input and output without using sequencealigned\n",
    "RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
    "self-attention and discuss its advantages over models such as [17, 18] and [9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most competitive neural sequence transduction models have an encoder-decoder structure [(cite)](cho2014learning,bahdanau2014neural,sutskever14). Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1,...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [(cite)](graves2013generating), consuming the previously generated symbols as additional input when generating the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base model for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        memory = self.encoder(self.src_embed(src), src_mask)\n",
    "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ModalNet-21.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Stacks   \n",
    "\n",
    "### Encoder: \n",
    "\n",
    "The encoder is composed of a stack of $N=6$ identical layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ a residual connection [(cite)](he2016deep) around each of the two sub-layers, followed by layer normalization [(cite)](layernorm2016).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout [(cite)](srivastava2014dropout) to the output of each sub-layer, before it is added to the sub-layer input and normalized.  \n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity we apply the norm first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder:\n",
    "\n",
    "The decoder is also composed of a stack of $N=6$ identical layers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ba5323ed320>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEyCAYAAACMONd1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT1KT/h8Lhr1O/TBL2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba53c5b6f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The attention mask shows the position each tgt word (row) is allowed to look at (column).\n",
    "# Words are blocked for attending to future words during training. \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention:                                                                                                                                                                                                                                                                               \n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.                                                                                                                                                                                                                                                                                           \n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\".   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.                                                                                                         \n",
    "<img width=\"220px\" src=\"ModalNet-19.png\">\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      \n",
    "                                                                 \n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$                                                                                                                                                                                                        \n",
    "                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=0.0):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    # (Dropout described below)\n",
    "    p_attn = F.dropout(p_attn, p=dropout)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most commonly used attention functions are additive attention [(cite)](bahdanau2014neural), and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.                                                                                             \n",
    "\n",
    "                                                                        \n",
    "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [(cite)](DBLP:journals/corr/BritzGLL17). We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Instead of performing a single attention function with $d_{\\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively.                                                                                                                                                                                                   \n",
    "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values:\n",
    "\n",
    "<img width=\"270px\" src=\"ModalNet-20.png\">\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.                                                                                                                                                                                                                                                                                             \n",
    "    \n",
    "    \n",
    "   \n",
    "$$    \n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.                                                                                                                                                                                                                                                        \n",
    "   \n",
    "\n",
    "   \n",
    "In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.p = dropout\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Attention in our Model                                                                                                                                                      \n",
    "The Transformer uses multi-head attention in three different ways:                                                        \n",
    "1) In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [(cite)](wu2016google, bahdanau2014neural,JonasFaceNet2017).    \n",
    "\n",
    "\n",
    "2) The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   \n",
    "\n",
    "\n",
    "3) Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.                                                                                                                                                                                                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks                                                                                                                                                                                                                                                                                                                                                             \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                        \n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # Torch linears have a `b` by default. \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [(cite)](press2016using). In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$.                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding                                                                                                                             \n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](JonasFaceNet2017). \n",
    "\n",
    "In this work, we use sine and cosine functions of different frequencies:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "$$                                                                                                                                                                                                                                                        \n",
    "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. \n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. \n",
    "                                                                                                                                                                                                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAEyCAYAAACh2dIXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba53c50f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The positional encoding will add in a sine wave based on position.\n",
    "# The frequency and offset of the wave is different for each dimension.\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also experimented with using learned positional embeddings [(cite)](JonasFaceNet2017) instead, and found that the two versions produced nearly identical results.  We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Standard generation step. (Not described in the paper.)\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct a model object based on hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small example model.\n",
    "tmp_model = make_model(10, 10, 2)\n",
    "tmp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section describes the training regime for our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Batching\n",
    "\n",
    "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.  Sentences were encoded using byte-pair encoding \\citep{DBLP:journals/corr/BritzGLL17}, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [(cite)](wu2016google). \n",
    "\n",
    "\n",
    "Sentence pairs were batched together by approximate sequence length.  Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware and Schedule                                                                                                                                                                                                   \n",
    "We trained our models on one machine with 8 NVIDIA P100 GPUs.  For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.  We trained the base models for a total of 100,000 steps or 12 hours.  For our big models, step time was 1.0 seconds.  The big models were trained for 300,000 steps (3.5 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "We used the Adam optimizer [(cite)](kingma2014adam) with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$.  We varied the learning rate over the course of training, according to the formula:                                                                                            \n",
    "$$                                                                                                                                                                                                                                                                                         \n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot                                                                                                                                                                                                                                                                                                \n",
    "  \\min({step\\_num}^{-0.5},                                                                                                                                                                                                                                                                                                  \n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})                                                                                                                                                                                                                                                                               \n",
    "$$                                                                                                                                                                                             \n",
    "This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.  We used $warmup\\_steps=4000$.                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: This part is incredibly important. \n",
    "# Need to train with this setup of the model is very unstable.\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCEiIhEpUYiISET/DwHlDSBsc/hLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba53c3423c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "                                                                                                                                                                                                                                                                                                                      \n",
    "### Label Smoothing\n",
    "\n",
    "During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ [(cite)](DBLP:journals/corr/SzegedyVISW15).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAADsCAYAAAB9hnEqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktQoAyBJjfo/udY5UFCxFaQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba53d38ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example\n",
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "plt.imshow(crit.true_dist)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ba53d911550>]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD9CAYAAABHnDf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGghJREFUeJzt3XuUXeV53/Hvc25z5iaNNBKgO3IrIwswF0+ANCTFcVwk\nmmXFWemKcFIS14TSGtfx6loxrtdqV5N2NWkuK24NVlVKTRzHJHGIoziycRob240NRdwlQCAEkkZC\n1gVdZjSXc3v6x95ntGd0zsyRdGbO7H1+n8Usnb3PqznPq8tPL+/e77vN3RERkWRJtboAERFpPoW7\niEgCKdxFRBJI4S4ikkAKdxGRBFK4i4gk0IzhbmYPm9lRM9tV530zs/9mZnvN7EUzu7H5ZYqIyIVo\nZOT+RWDjNO9vAtaFX/cAX7j0skRE5FLMGO7u/j3gnWmabAb+yANPAn1mtqxZBYqIyIVrxpz7CuBg\n5HgwPCciIi2SmcsPM7N7CKZu6O7uft/69evn8uNFRGLvmWeeOe7uS2dq14xwPwSsihyvDM+dx923\nAdsABgYGfOfOnU34eBGR9mFm+xtp14xpme3AXeFdM7cAp9397SZ8XxERuUgzjtzN7CvAbcASMxsE\n/gOQBXD3rcAO4A5gLzACfHS2ihURkcbMGO7ufucM7zvw8aZVJCIil0wrVEVEEkjhLiKSQAp3EZEE\nUriLiCRQ7MJ9z5Ehfv9bezgxPN7qUkRE5q3Yhfu+Y8P892/v5eiQwl1EpJ7YhXs+mwZgrFhucSUi\nIvNXbMN9VOEuIlJXDMM9KHm8WGlxJSIi81fswr0zp5G7iMhMYhfu+Yzm3EVEZhK7cNfIXURkZrEL\n93Mjd825i4jUE79wzwUla1pGRKS+2IV7Lp0iZTBaULiLiNQTu3A3M/LZtEbuIiLTiF24A3Rm07qg\nKiIyjViGezBy1wVVEZF6YhruKU3LiIhMI6bhrjl3EZHpxDLcNecuIjK9WIa7Ru4iItOLbbiP6oKq\niEhdMQ33FOMauYuI1BXLcNecu4jI9OIZ7jmFu4jIdGIZ7rqgKiIyvRiHewV3b3UpIiLzUkzDPXyO\nakl3zIiI1BLLcO/Mhk9j0ra/IiI1xTLc82G4j5UU7iIitcQy3DVyFxGZXizDvTrnrm1/RURqayjc\nzWyjme0xs71mdn+N9xea2V+b2QtmttvMPtr8Us+pTsvoXncRkdpmDHczSwMPAJuADcCdZrZhSrOP\nAy+7+3XAbcDvm1muybVOmJhzV7iLiNTUyMj9JmCvu+9z9wLwKLB5ShsHes3MgB7gHaDU1EojOhXu\nIiLTaiTcVwAHI8eD4bmozwPvAQ4DLwGfdPdZmxDvzGlaRkRkOs26oHo78DywHLge+LyZLZjayMzu\nMbOdZrbz2LFjF/1h+Ux15K4LqiIitTQS7oeAVZHjleG5qI8Cj3lgL/AmsH7qN3L3be4+4O4DS5cu\nvdiayeeCsjVyFxGprZFwfxpYZ2Zrw4ukW4DtU9ocAD4AYGaXA1cB+5pZaFT1gqr2dBcRqS0zUwN3\nL5nZfcDjQBp42N13m9m94ftbgd8CvmhmLwEGfNrdj89W0VrEJCIyvRnDHcDddwA7ppzbGnl9GPgn\nzS2tvmw6RTpl2n5ARKSOWK5QhfBpTAVdUBURqSW24Z7PpjRyFxGpI8bhnmZMc+4iIjXFOtx1K6SI\nSG2xDfdOPUdVRKSuWIe7Ru4iIrXFNtw7siltPyAiUkdsw13TMiIi9cU23PMKdxGRumIb7ppzFxGp\nL7bhntecu4hIXfEN95xG7iIi9cQ33DNpCqUK5Yq3uhQRkXkntuFefdTeuPaXERE5T2zDPZ8Jn8ak\n/WVERM4T23CvjtzHSrqoKiIyVWzDPa+nMYmI1BX7cNdCJhGR88U23DsV7iIidcU23M+N3DXnLiIy\nVWzDvTpy10ImEZHzxTbc89mgdE3LiIicL8bhrpG7iEg9sQ93jdxFRM4X23CfWMSkcBcROU9sw/3c\n9gO6W0ZEZKrYhnsmnSKbNsa0cZiIyHliG+4QbPur7QdERM4X73DPpbXlr4hIDbEO986sRu4iIrXE\nOtz1HFURkdpiHe6dWT1HVUSklobC3cw2mtkeM9trZvfXaXObmT1vZrvN7LvNLbO2jmxa97mLiNSQ\nmamBmaWBB4APAoPA02a23d1fjrTpAx4ENrr7ATO7bLYKjurMpjk1UpiLjxIRiZVGRu43AXvdfZ+7\nF4BHgc1T2nwEeMzdDwC4+9HmlllbPpvStIyISA2NhPsK4GDkeDA8F/VuYJGZPWFmz5jZXc0qcDqd\n2bQuqIqI1DDjtMwFfJ/3AR8AOoEfmtmT7v5atJGZ3QPcA7B69epL/tC8LqiKiNTUyMj9ELAqcrwy\nPBc1CDzu7mfd/TjwPeC6qd/I3be5+4C7DyxduvRia56Q1wVVEZGaGgn3p4F1ZrbWzHLAFmD7lDZ/\nBdxqZhkz6wJuBl5pbqnnU7iLiNQ247SMu5fM7D7gcSANPOzuu83s3vD9re7+ipl9E3gRqAAPufuu\n2Swcgjn3YtkplStk0rG+ZV9EpKkamnN39x3Ajinntk45/l3gd5tX2sw6c+Gj9koVehTuIiITYp2I\nehqTiEhtiQh3bR4mIjJZIsJdI3cRkcliHe6dE+GuhUwiIlGxDvd8NnyOqkbuIiKTxDrcu3LByH2k\nUGpxJSIi80usw72vKwfASe0MKSIySazDvb87CPcTwwp3EZGoWIf7gnyWdMp456zCXUQkKtbhnkoZ\ni7pyCncRkSliHe4QTM2cULiLiEwS+3Bf3K2Ru4jIVPEP9x6Fu4jIVLEP9/7uHCeGx1tdhojIvBL7\ncF/cnePMWIliWVsQiIhUxT7cq/e6ayGTiMg5sQ/3xd0dAJp3FxGJSEC4ByP3d7RKVURkQuzDvb8n\n3IJAI3cRkQmxD/eJkbvCXURkQuzDva8zC2jkLiISFftwz6RT9HVleees7nUXEamKfbiDtiAQEZkq\nEeEerFJVuIuIVCUi3DVyFxGZLCHh3qFwFxGJSES493fnODlSoFLxVpciIjIvJCLcF3fnqDicGi22\nuhQRkXkhEeFeXaWq2yFFRAKJCPfqKlXdMSMiEkhUuOuiqohIIBHh3h9u+6stCEREAokI90Xdwf4y\nGrmLiAQaCncz22hme8xsr5ndP027HzOzkpn9QvNKnFlHJk1vR0bhLiISmjHczSwNPABsAjYAd5rZ\nhjrtfgf4VrOLbMTiHq1SFRGpamTkfhOw1933uXsBeBTYXKPdJ4C/AI42sb6GaQsCEZFzGgn3FcDB\nyPFgeG6Cma0APgx8oXmlXZj+7pwuqIqIhJp1QfUPgU+7e2W6RmZ2j5ntNLOdx44da9JHB4KRuxYx\niYgAZBpocwhYFTleGZ6LGgAeNTOAJcAdZlZy969FG7n7NmAbwMDAQFM3gqluHubuhHWIiLStRsL9\naWCdma0lCPUtwEeiDdx9bfW1mX0R+PrUYJ9t/d05imVnaLzEgnx2Lj9aRGTemXFaxt1LwH3A48Ar\nwJ+5+24zu9fM7p3tAhs1sUpVWxCIiDQ0csfddwA7ppzbWqftr156WRducbh52ImzBa5c0t2KEkRE\n5o1ErFCFYFoGtEpVRAQSFO5LeoL9ZY4N6Y4ZEZHEhPvlC/Lk0in2nzjb6lJERFouMeGeThmrFnfy\nlsJdRCQ54Q6wdkk3bx0faXUZIiItl6hwX9Pfzf53zupB2SLS9hIV7lcu6WasWOFHQ2OtLkVEpKUS\nFe5r+4P72zU1IyLtLlHhvqa/C0AXVUWk7SUq3Jf3dZJLpxTuItL2EhXuE7dDHle4i0h7S1S4Q3A7\n5P4TmnMXkfaWuHBf09/NWyd0O6SItLfEhXv1dsij2mNGRNpY8sI9vGPmTc27i0gbS2C4B/e6awMx\nEWlniQv36u2QbyrcRaSNJS7cq7dD7tcqVRFpY4kLdwimZrSQSUTaWTLDfUkQ7u66HVJE2lMyw72/\nK9gd8oxuhxSR9pTMcF8S7g6pqRkRaVPJDPeJrX8V7iLSnhIZ7sv7OunMpnn1yFCrSxERaYlEhns6\nZbx35UKeO3Cy1aWIiLREIsMd4MY1i9h9+AxjxXKrSxERmXPJDffViyhVnJcOnW51KSIicy6x4X7D\n6j4Ant2vqRkRaT+JDfclPR2sXtzFs5p3F5E2lNhwB7hxdR/PHjillaoi0naSHe5rFnFsaJzBk6Ot\nLkVEZE4lO9xXLwLguYOnWlyJiMjcaijczWyjme0xs71mdn+N93/JzF40s5fM7Admdl3zS71w66/o\npTOb1kVVEWk7M4a7maWBB4BNwAbgTjPbMKXZm8A/dvdrgd8CtjW70IuRSae0mElE2lIjI/ebgL3u\nvs/dC8CjwOZoA3f/gbtXE/RJYGVzy7x4WswkIu2okXBfARyMHA+G5+r5GPCNSymqmbSYSUTaUVMv\nqJrZ+wnC/dN13r/HzHaa2c5jx44186Pr0mImEWlHjYT7IWBV5HhleG4SM3sv8BCw2d1P1PpG7r7N\n3QfcfWDp0qUXU+8FW9LTwbuWdvP914/PyeeJiMwHjYT708A6M1trZjlgC7A92sDMVgOPAf/c3V9r\nfpmX5varr+CH+05waqTQ6lJERObEjOHu7iXgPuBx4BXgz9x9t5nda2b3hs3+PdAPPGhmz5vZzlmr\n+CJsuuYKyhXnb1/+UatLERGZE5lGGrn7DmDHlHNbI6/vBu5ubmnNc+2Khazo6+Sbu47wzwZWzfwT\nRERiLtErVKvMjNuvvoLvv36c4fFSq8sREZl1bRHuAJuuvYJCucK3Xz3a6lJERGZd24T7jasXsaSn\ng8d3HWl1KSIis65twj2dMm6/+nK+s+eoVquKSOK1TbgDbLpmGSOFMt99bW4WUImItEpbhfvN71rM\nws4sf/Pi260uRURkVrVVuGfTKT58wwq+settfnRmrNXliIjMmrYKd4B/8RNrKVWcR37wVqtLERGZ\nNW0X7qv7u7h9wxV8+akDjBR0z7uIJFPbhTvAr/3UWk6PFvnznYOtLkVEZFa0ZbjfuHoR16/q4+G/\nf5NyxVtdjohI07VluJsZv/aT72L/iRFtJiYiidSW4Q5w+9WXs3JRJ194Yi8Vjd5FJGHaNtwz6RS/\n/jPv5oXB03z1Wc29i0iytG24A/z8DSsYWLOI3/7Gq5weKba6HBGRpmnrcE+ljN/cfA2nRgr83rf2\ntLocEZGmaetwB9iwfAF3/fiV/PFT+9l16HSryxERaYq2D3eAT33w3fR35/jsX75EoVRpdTkiIpdM\n4Q4s7Mzym5uv4YXB0/znv3m51eWIiFwyhXvojmuXcfeta3nkh/t5THfPiEjMKdwj7t+0nlvetZjP\nPPYSuw9r/l1E4kvhHpFJp/j8R25kUVeOf/mlZzh8arTVJYmIXBSF+xRLejrYdtf7OD1SZMu2Jzmk\ngBeRGFK41/DelX186e6bOTlSYMu2HzJ4cqTVJYmIXBCFex3Xr+rjy3ffzOmRIr/4P57klbfPtLok\nEYkpd2esWObE8DgH3xnh+PD4rH+mubdm06yBgQHfuXNnSz77Quw6dJqPPfI0p0eL/Jefv5YP37Cy\n1SWJyCwrV5yzhRJnx6tf5eDHQvXHqedrvC6UGImci+5P+K9u+wd8euP6i6rNzJ5x94GZ2mUu6ru3\nkWtWLOSvP3Er9/3Jc3zqT1/guQOn+Hd3vId8Nt3q0kQk5O6MFssMj5cYHgvCdbgazIUSQ2Pngnp4\n/PyAHo4cD4+XGCs2vpixK5emuyNDT0dm4vWSnhyrc110dwTHk9tkeM+y3ln81Qgo3BtwWW+eL999\nM//1m6/yP7//Jt997Rj/6eeu4SfXLW11aSKxNRHIY0GgVoO5+vrseImhaiCPBaE8PF7k7Hh50vla\nI+PpdEeCtrsjQ3dHmuV9+fD1uZCuBnF3R3qibTTAuzsydGXTpFI2u79QF0nTMhfoB3uP89mv7eLN\n42fZfP1yfmPjelb0dba6LJE5U674lDAuMjQ2OZzPO54U0iWGxooMjzcWyOmU0TMRxpODdtLrfPV1\nmp6ObDBqzgXne2IQxo1qdFpG4X4RxoplHnziDbY+8QaO84s/top/fds/ZLlCXuYxd+dsIRgpD40V\nGaqG8Ni5sD0z5bga1NXjobESI4VyQ5/XnUtPBGtPPktvGM69+exEME+8X+s4fN2RSWEW70BuJoX7\nHBg8OcKDT7zBn+88iGH87HXL+KWb13Dj6j79YZSmKpUrYciWOBMJ2qGx4rkfJ86FYTx2bgRd/TmN\n/HXvDUe5vfngq7sjw4JqIIeBG32vGta9+cgIOpchHfMR8nylcJ9DgydH2Pa9fTz27CGGx0usv6KX\nX3jfSjZdu0xTNkKhVImE8LkgPjM6+Vw1sM+Ebc9EAnq0OPNoOZdJsSCfmTQy7g2Pe/MZFuSr4Zyd\nCOfeaPt8hp5cJvbTFkmncG+Bs+Mltr9wmD956gAvhXvDX7eqjw++5zJuXbeUa1cs1GgmZmoF85nI\niPlMZOR8Lpwj748WGW9gG+nqBbxoGAeBnJ1yLgjiBZFzPWHbjozu4GoHTQ13M9sIfA5IAw+5+29P\ned/C9+8ARoBfdfdnp/ueSQz3qP0nzrLjpSN8Y9fbvDgYBP3Cziw3rV3MDav7uGHVIq5duZCeDt2w\nNBvcnZFCOZyWqAby5OmKofHJ0xrDU6Y1zoyVGtrfvyuXPi+AqyPl3nCuufb7WRZ0BiPsTFrrCaUx\nTQt3M0sDrwEfBAaBp4E73f3lSJs7gE8QhPvNwOfc/ebpvm/Swz3qxPA4f//GCf7v68fY+dZJ9h0/\nO/HeqsWdXHX5Aq66oocr+7tZu6SbNf3dLOnJtd28fbCKrzLpfuPqwpCpd1tEb5WbdAFw7Fzbme7E\nMIOe3NR55HMBvCBfO5SjI2oFs8y1Zi5iugnY6+77wm/8KLAZiD7VYjPwRx78S/GkmfWZ2TJ3f/si\nak+c/p4OPnTdcj503XIATp4t8PzgKXYfOs0rR4bYc2SI7+w5SjmSRrlMimUL81yxIM/S3g6W9naw\npKeDRV05FnZm6euafIGrM5cmn0mTTdus/KNQqTiFcoXxUoVCqcJ4qcx4qcJYscxYscJ4scxYKXg9\nWigzWiwzViwzUgi+RgvBXRYjxTKj4Sq/kcL0q/imU71XOZiWyNLTkWZpb0fN+ebqtEVvx+SpDM0v\nS5I1Eu4rgIOR40GC0flMbVYACvcaFnXneP9Vl/H+qy6bOFcsVxg8OcpbJ85y4MQIh0+PcvjUGEdO\nj7L78BmODY0zPF6a8XunU0YunSKXSZFNp8imjXQq/DKD4D/MjIo7BP9RrjjliuPuFMPXxXKFYrlC\nqeyUGk3dGrJpozObpiuXoasjTVcueB1dxVddLDKxuCSXiSw0SU/cmdHdoTsxRBoxpxO+ZnYPcA/A\n6tWr5/Kj571sOsXaJcG0TD1jxTKnR4ucGilyaqRwbiHJeInRQjBSHi2WKZQqFMvOeKlCqVyh7H4u\nvCEMdMfMJoI+bZBKGSkzsmkjk0qRThkd4T8SmbSRy6TIpVN0ZFJ0ZNPBj+HrfCZNPpuiM5emMxt8\n5cPXWU1biMy5RsL9ELAqcrwyPHehbXD3bcA2CObcL6hSIZ9Nk8+muXxBvtWliMg818iQ6mlgnZmt\nNbMcsAXYPqXNduAuC9wCnNZ8u4hI68w4cnf3kpndBzxOcCvkw+6+28zuDd/fCuwguFNmL8GtkB+d\nvZJFRGQmDc25u/sOggCPntsaee3Ax5tbmoiIXCxd6RIRSSCFu4hIAincRUQSSOEuIpJACncRkQRq\n2Za/ZnYM2H8BP2UJcHyWypnP2rXf0L59V7/by4X2e427z/gA55aF+4Uys52N7ISWNO3ab2jfvqvf\n7WW2+q1pGRGRBFK4i4gkUJzCfVurC2iRdu03tG/f1e/2Miv9js2cu4iINC5OI3cREWlQLMLdzDaa\n2R4z22tm97e6ntliZqvM7Dtm9rKZ7TazT4bnF5vZ35rZ6+GPi1pd62wws7SZPWdmXw+PE9/v8JGU\nXzWzV83sFTP78Tbp96fCP+O7zOwrZpZPYr/N7GEzO2pmuyLn6vbTzD4T5tweM7v9Uj573od7+IDu\nB4BNwAbgTjPb0NqqZk0J+LfuvgG4Bfh42Nf7gb9z93XA34XHSfRJ4JXIcTv0+3PAN919PXAdQf8T\n3W8zWwH8G2DA3a8h2Ep8C8ns9xeBjVPO1exn+Hd9C3B1+HMeDPPvosz7cCfygG53LwDVB3Qnjru/\n7e7Phq+HCP6iryDo7yNhs0eAn2tNhbPHzFYC/xR4KHI60f02s4XATwH/C8DdC+5+ioT3O5QBOs0s\nA3QBh0lgv939e8A7U07X6+dm4FF3H3f3Nwmej3HTxX52HMK93sO3E83MrgRuAJ4CLo882eoIcHmL\nyppNfwj8BlCJnEt6v9cCx4D/HU5HPWRm3SS83+5+CPg94ADwNsGT275FwvsdUa+fTc26OIR72zGz\nHuAvgF939zPR98IHoyTqFicz+1ngqLs/U69NEvtNMHq9EfiCu98AnGXKVEQS+x3OMW8m+MdtOdBt\nZr8cbZPEftcym/2MQ7g39PDtpDCzLEGwf9ndHwtP/8jMloXvLwOOtqq+WfITwIfM7C2CabefNrM/\nJvn9HgQG3f2p8PirBGGf9H7/DPCmux9z9yLwGPCPSH6/q+r1s6lZF4dwb+QB3YlgZkYw//qKu/9B\n5K3twK+Er38F+Ku5rm02uftn3H2lu19J8Pv7bXf/ZZLf7yPAQTO7Kjz1AeBlEt5vgumYW8ysK/wz\n/wGC60tJ73dVvX5uB7aYWYeZrQXWAf/voj/F3ef9F8HDt18D3gA+2+p6ZrGftxL8L9qLwPPh1x1A\nP8FV9deB/wMsbnWts/hrcBvw9fB14vsNXA/sDH/PvwYsapN+/0fgVWAX8CWgI4n9Br5CcF2hSPB/\nah+brp/AZ8Oc2wNsupTP1gpVEZEEisO0jIiIXCCFu4hIAincRUQSSOEuIpJACncRkQRSuIuIJJDC\nXUQkgRTuIiIJ9P8BXL4fTR1k0e8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ba53d29c828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Label smoothing starts to penalize the model \n",
    "# if it gets very confident about a given choice\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_backprop(generator, criterion, out, targets, normalize):\n",
    "    \"\"\"\n",
    "    Memory optmization. Compute each timestep separately and sum grads.\n",
    "    \"\"\"\n",
    "    assert out.size(1) == targets.size(1)\n",
    "    total = 0.0\n",
    "    out_grad = []\n",
    "    for i in range(out.size(1)):\n",
    "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
    "        gen = generator(out_column)\n",
    "        loss = criterion(gen, targets[:, i]) / normalize\n",
    "        total += loss.data[0]\n",
    "        loss.backward()\n",
    "        out_grad.append(out_column.grad.data.clone())\n",
    "    out_grad = torch.stack(out_grad, dim=1)\n",
    "    out.backward(gradient=out_grad)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_std_mask(src, tgt, pad):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "                        \n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        if i % 10 == 1:\n",
    "            print(i, loss, model_opt._rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
    "    model.test()\n",
    "    total = 0\n",
    "    for batch in valid_iter:\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_mask = src_mask\n",
    "        self.trg_mask = trg_mask\n",
    "        self.ntokens = ntokens\n",
    "    \n",
    "def data_gen(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
    "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.7470001578330994 6.987712429686844e-07\n",
      "11 2.5154976844787598 4.192627457812107e-06\n",
      "1 2.5138686299324036 7.686483672655528e-06\n",
      "11 2.257762908935547 1.118033988749895e-05\n",
      "1 2.105326920747757 1.4674196102342371e-05\n",
      "11 1.9795219749212265 1.8168052317185794e-05\n",
      "1 1.8961711823940277 2.1661908532029216e-05\n",
      "11 1.8223628401756287 2.515576474687264e-05\n",
      "1 1.7691601067781448 2.8649620961716057e-05\n",
      "11 1.6932887583971024 3.214347717655948e-05\n",
      "1 1.7032272815704346 3.56373333914029e-05\n",
      "11 1.6878212541341782 3.913118960624633e-05\n",
      "1 1.6262404024600983 4.262504582108975e-05\n",
      "11 1.6096591800451279 4.611890203593317e-05\n",
      "1 1.579153761267662 4.961275825077659e-05\n",
      "11 1.619109869003296 5.310661446562001e-05\n",
      "1 1.4801566004753113 5.660047068046343e-05\n",
      "11 1.5084412023425102 6.0094326895306855e-05\n",
      "1 1.4217040985822678 6.358818311015028e-05\n",
      "11 1.3820594772696495 6.70820393249937e-05\n",
      "1 1.3241027146577835 7.057589553983712e-05\n",
      "11 1.3226915001869202 7.406975175468054e-05\n",
      "1 1.2149531245231628 7.756360796952397e-05\n",
      "11 1.0942132845520973 8.10574641843674e-05\n",
      "1 1.0154631361365318 8.455132039921081e-05\n",
      "11 0.9320393353700638 8.804517661405423e-05\n",
      "1 0.8724211677908897 9.153903282889765e-05\n",
      "11 0.7049228474497795 9.503288904374107e-05\n",
      "1 0.6676593869924545 9.85267452585845e-05\n",
      "11 0.6228175796568394 0.00010202060147342792\n"
     ]
    }
   ],
   "source": [
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = get_std_opt(model)\n",
    "for epoch in range(15):\n",
    "    train_epoch(data_gen(V, 30, 20), model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load words from IWSLT\n",
    "\n",
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "MAX_LEN = 100\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 1\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Detail. Batching seems to matter quite a bit. \n",
    "# This is temporary code for dynamic batching based on number of tokens.\n",
    "# This code should all go away once things get merged in this library.\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
    "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
    "\n",
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(131945, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(57973, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=57973)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model an load it onto our GPU.\n",
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model_opt = get_std_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9.299771845340729 6.987712429686844e-07\n",
      "11 9.415135336574167 4.192627457812107e-06\n",
      "21 8.813630282878876 7.686483672655528e-06\n",
      "31 9.112178653478622 1.118033988749895e-05\n",
      "41 8.607461810112 1.4674196102342371e-05\n",
      "51 8.913826749660075 1.8168052317185794e-05\n",
      "61 8.701497752219439 2.1661908532029216e-05\n",
      "71 8.373274087905884 2.515576474687264e-05\n",
      "81 8.454237446188927 2.8649620961716057e-05\n",
      "91 7.6996782422065735 3.214347717655948e-05\n",
      "101 8.037408232688904 3.56373333914029e-05\n",
      "111 7.704962134361267 3.913118960624633e-05\n",
      "121 7.699015600606799 4.262504582108975e-05\n",
      "131 7.367554426193237 4.611890203593317e-05\n",
      "141 7.2071177661418915 4.961275825077659e-05\n",
      "151 7.106400920893066 5.310661446562001e-05\n",
      "161 6.804656069725752 5.660047068046343e-05\n",
      "171 6.390337720513344 6.0094326895306855e-05\n",
      "181 5.687528342008591 6.358818311015028e-05\n",
      "191 6.122820109128952 6.70820393249937e-05\n",
      "201 5.829070374369621 7.057589553983712e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
    "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field()\n",
    "TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 100\n",
    "train = datasets.TranslationDataset(path=\"/n/home00/srush/Data/baseline-1M_train.tok.shuf\", \n",
    "                                    exts=('.en', '.fr'),\n",
    "                                    fields=(SRC, TGT), \n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "SRC.build_vocab(train.src, max_size=50000)\n",
    "TGT.build_vocab(train.trg, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50002, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50004, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=50004)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "print(pad_idx)\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), pad_idx, N=6)\n",
    "model_opt = get_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)\n",
    "    valid_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50002\n"
     ]
    }
   ],
   "source": [
    "print(pad_idx)\n",
    "print(len(SRC.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"/n/rush_lab/trans_ipython.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.269582842476666 0.0005377044714644026\n",
      "101 3.300532897672383 0.0005726430336128369\n",
      "201 3.3047672072425485 0.0006075815957612711\n",
      "301 2.7151080842595547 0.0006425201579097052\n",
      "401 2.6975380268413574 0.0006774587200581396\n",
      "501 3.051631387323141 0.0007123972822065737\n",
      "601 2.554425454698503 0.000747335844355008\n",
      "701 2.6254820519825444 0.0007822744065034422\n",
      "801 2.868743653933052 0.0008172129686518764\n",
      "901 2.5978208642918617 0.0008521515308003106\n",
      "1001 2.5955790174775757 0.0008870900929487448\n",
      "1101 2.6764775353949517 0.000922028655097179\n",
      "1201 2.464000296778977 0.0009569672172456132\n",
      "1301 2.0503073083236814 0.0009919057793940475\n",
      "1401 2.295472824771423 0.0010268443415424816\n",
      "1501 2.245281406212598 0.0010617829036909158\n",
      "1601 2.2577588511630893 0.00109672146583935\n",
      "1701 2.2232908592559397 0.0011316600279877844\n",
      "1801 2.357596361427568 0.0011665985901362186\n",
      "1901 2.121352154412307 0.0012015371522846527\n",
      "2001 2.5742998471250758 0.001236475714433087\n",
      "2101 2.2518509055953473 0.0012714142765815214\n",
      "2201 2.2251326659170445 0.0013063528387299553\n",
      "2301 2.078994876006618 0.0013412914008783896\n",
      "2401 2.068276036065072 0.001376229963026824\n",
      "2501 2.31435151558253 0.0013907788851585368\n",
      "2601 1.9106871648691595 0.0013738752565588634\n",
      "2701 2.183084836578928 0.0013575733592730722\n",
      "2801 2.4668076275847852 0.0013418383196400342\n",
      "2901 1.963176985620521 0.0013266380295186675\n",
      "3001 2.2140520309330896 0.0013119428705609764\n",
      "3101 2.6989458349489723 0.0012977254713568687\n",
      "3201 2.1293521663174033 0.0012839604929174666\n",
      "3301 2.1402786187827587 0.0012706244386700126\n",
      "3401 2.041781216394156 0.0012576954857216498\n",
      "3501 2.051893091876991 0.0012451533346344698\n",
      "3601 1.5498304846696556 0.001232979075358713\n",
      "3701 2.763939742697403 0.001221155067309524\n",
      "3801 2.7611468499198963 0.0012096648318570434\n",
      "3901 1.7321470333263278 0.0011984929557393293\n",
      "4001 2.139603299088776 0.0011876250041103701\n",
      "4101 2.1966493157087825 0.0011770474421074978\n",
      "4201 2.0962203710805625 0.0011667475639689723\n",
      "4301 1.9717675620922819 0.0011567134288575545\n",
      "4401 2.097687987901736 0.0011469338026529508\n",
      "4501 1.9319786678534001 0.001137398105067946\n",
      "4601 1.8846281475271098 0.0011280963615221983\n",
      "4701 1.9817245414596982 0.0011190191592759865\n",
      "4801 1.7659185670199804 0.0011101576073853326\n",
      "4901 2.188665813198895 0.0011015033000912066\n",
      "5001 2.1391192222399695 0.0010930482833001135\n",
      "5101 1.8125874139368534 0.0010847850238522342\n",
      "5201 1.6616800595074892 0.0010767063813072288\n",
      "5301 1.6544548005331308 0.0010688055820075176\n",
      "5401 1.9542939933016896 0.0010610761952049212\n",
      "5501 2.218412609123334 0.0010535121110594244\n",
      "5601 1.838119359650591 0.001046107520339004\n",
      "5701 1.892627771012485 0.0010388568956672375\n",
      "5801 2.2462481096954434 0.0010317549741811346\n",
      "5901 1.4471426841337234 0.0010247967414755423\n",
      "6001 1.9312338004237972 0.0010179774167228303\n",
      "6101 1.7303275546291843 0.001011292438867507\n",
      "6201 1.8833909621462226 0.0010047374538051973\n",
      "6301 1.8943474531406537 0.0009983083024640838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5940000533591956 0.0009927515780513657\n",
      "101 1.7524283765815198 0.0009865483707369156\n",
      "201 1.900138527940726 0.0009804600111146078\n",
      "301 1.8419977760640904 0.0009744829985071481\n",
      "401 1.9621913449373096 0.0009686139798247046\n",
      "501 2.226916428655386 0.0009628497416600543\n",
      "601 1.7190162097394932 0.0009571872028951208\n",
      "701 1.8589106332874508 0.0009516234077802563\n",
      "801 1.8107321247807704 0.000946155519450957\n",
      "901 1.6531266793608665 0.0009407808138497059\n",
      "1001 1.4840005157748237 0.0009354966740233614\n",
      "1101 1.7578403616789728 0.0009303005847689719\n",
      "1201 1.3920216620899737 0.0009251901276031373\n",
      "1301 1.6626927084289491 0.0009201629760320567\n",
      "1401 1.7256765578058548 0.0009152168911012566\n",
      "1501 1.6049046433763579 0.0009103497172056578\n",
      "1601 1.6955451717367396 0.000905559378142174\n",
      "1701 1.6796367820352316 0.0009008438733884249\n",
      "1801 1.5794002648835885 0.0008962012745924116\n",
      "1901 1.9637197174597532 0.0008916297222591652\n",
      "2001 1.4656428614398465 0.0008871274226214399\n",
      "2101 1.567156056407839 0.0008826926446824871\n",
      "2201 1.542241255287081 0.0008783237174198395\n",
      "2301 1.690121710913445 0.0008740190271398465\n",
      "2401 1.357302049640566 0.0008697770149734477\n",
      "2501 1.9049871656461619 0.0008655961745043597\n",
      "2601 2.240402895025909 0.0008614750495214811\n",
      "2701 1.7940634173137369 0.0008574122318878972\n",
      "2801 1.7314323161263019 0.0008534063595194054\n",
      "2901 1.6064868164248765 0.0008494561144659686\n",
      "3001 1.7515187719254754 0.0008455602210899614\n",
      "3101 1.552100334316492 0.0008417174443354889\n",
      "3201 1.6221882179379463 0.0008379265880834463\n",
      "3301 1.5139061958470847 0.0008341864935873445\n",
      "3401 1.6668659402348567 0.0008304960379852562\n",
      "3501 2.1993618682026863 0.0008268541328835436\n",
      "3601 1.823760490231507 0.0008232597230083089\n",
      "3701 1.8189842144493014 0.0008197117849207771\n",
      "3801 1.689056838164106 0.0008162093257930558\n",
      "3901 1.5656833801185712 0.0008127513822409492\n",
      "4001 1.5621904337021988 0.0008093370192107105\n",
      "4101 1.4836799805052578 0.0008059653289168093\n",
      "4201 1.47899504378438 0.000802635429827976\n",
      "4301 1.6922758186701685 0.0007993464656989501\n",
      "4401 1.636858390578709 0.000796097604645519\n",
      "4501 1.5558803144613194 0.0007928880382605766\n",
      "4601 1.5102424336364493 0.00078971698076907\n",
      "4701 1.541241532890126 0.0007865836682198282\n",
      "4801 1.5931309935403988 0.000783487357712386\n",
      "4901 1.2315586884506047 0.0007804273266570247\n",
      "5001 1.527937745093368 0.0007774028720663579\n",
      "5101 1.31743333209306 0.0007744133098768835\n",
      "5201 1.5960889644484269 0.0007714579742990187\n",
      "5301 1.4181096099782735 0.0007685362171942096\n",
      "5401 1.4596448407392018 0.0007656474074777987\n",
      "5501 1.4594163084111642 0.0007627909305463981\n",
      "5601 1.62109798315214 0.0007599661877285873\n",
      "5701 1.586864550015889 0.0007571725957578231\n",
      "5801 1.5062829439411871 0.0007544095862665088\n",
      "5901 1.4292167258172412 0.0007516766053002225\n",
      "6001 1.4355267270930199 0.0007489731128511653\n",
      "6101 1.4162533966591582 0.0007462985824099354\n",
      "6201 1.6518787188415445 0.0007436525005347853\n",
      "6301 1.5916137372114463 0.0007410343664375577\n",
      "1 1.202994157327339 0.0007387531385993765\n",
      "101 1.4649722938484047 0.0007361862332058686\n",
      "201 1.1459896704182029 0.0007336459004644837\n",
      "301 1.417104929103516 0.0007311316850490442\n",
      "401 1.373963651509257 0.0007286431424819469\n",
      "501 1.6432027550181374 0.0007261798388040814\n",
      "601 1.4122836171882227 0.0007237413502569408\n",
      "701 1.6119428309611976 0.0007213272629763972\n",
      "801 1.5545603609643877 0.0007189371726976359\n",
      "901 1.5427279596333392 0.0007165706844707772\n",
      "1001 1.5437391183004365 0.0007142274123867243\n",
      "1101 1.9743895339342998 0.0007119069793128112\n",
      "1201 1.730805973522365 0.0007096090166378355\n",
      "1301 1.5635135210759472 0.0007073331640260875\n",
      "1401 1.206731209764257 0.000705079069180001\n",
      "1501 1.4495476994197816 0.0007028463876110714\n",
      "1601 1.2935033895773813 0.0007006347824187037\n",
      "1701 1.1734203454107046 0.000698443924076667\n",
      "1801 1.202259551268071 0.0006962734902268488\n",
      "1901 1.7874216835407424 0.0006941231654800159\n",
      "2001 1.5438914835685864 0.0006919926412233024\n",
      "2101 1.5168145569041371 0.000689881615434157\n",
      "2201 1.5306344364071265 0.0006877897925004977\n",
      "2301 1.5227781175635755 0.0006857168830468271\n",
      "2401 1.3308223116910085 0.0006836626037660786\n",
      "2501 1.4871021673316136 0.0006816266772569715\n",
      "2601 1.3415705130901188 0.0006796088318666611\n",
      "2701 1.2746119699440897 0.0006776088015384847\n",
      "2801 1.3439618053671438 0.0006756263256646049\n",
      "2901 1.3065503026737133 0.0006736611489433701\n",
      "3001 1.4918707825127058 0.0006717130212412112\n",
      "3101 1.4003087060991675 0.0006697816974589058\n",
      "3201 1.3473156996478792 0.0006678669374020495\n",
      "3301 1.3869949235959211 0.0006659685056555759\n",
      "3401 1.5086751837225165 0.000664086171462178\n",
      "3501 1.4735991460911464 0.00066221970860449\n",
      "3601 1.3997832712557283 0.0006603688952908887\n",
      "3701 1.5196008981074556 0.0006585335140447885\n",
      "3801 1.2834229312138632 0.0006567133515973014\n",
      "3901 1.3874705795169575 0.0006549081987831418\n",
      "4001 1.6422591609880328 0.0006531178504396635\n",
      "4101 1.305389653716702 0.0006513421053089143\n",
      "4201 1.5159487561322749 0.0006495807659426053\n",
      "4301 1.3981374967552256 0.0006478336386098913\n",
      "4401 1.7390631912276149 0.0006461005332078655\n",
      "4501 1.3604947600979358 0.0006443812631746732\n",
      "4601 1.7799529591429746 0.000642675645405156\n",
      "4701 1.3463407127128448 0.0006409835001689394\n",
      "4801 1.4632963918847963 0.0006393046510308797\n",
      "4901 1.1903231081087142 0.0006376389247737917\n",
      "5001 1.3287691511941375 0.0006359861513233783\n",
      "5101 1.3445309301023372 0.0006343461636752915\n",
      "5201 1.5431754024625661 0.0006327187978242499\n",
      "5301 1.3343850841192761 0.0006311038926951474\n",
      "5401 1.1768817943520844 0.0006295012900760858\n",
      "5501 1.6530805606771537 0.0006279108345532683\n",
      "5601 1.2646167293241888 0.000626332373447694\n",
      "5701 1.3651119051501155 0.000624765756753594\n",
      "5801 1.831987822048177 0.0006232108370785525\n",
      "5901 1.3451470380132378 0.0006216674695852594\n",
      "6001 1.5295006221767835 0.0006201355119348414\n",
      "6101 1.2796215488779126 0.0006186148242317232\n",
      "6201 1.3307579715619795 0.0006171052689699666\n",
      "6301 1.5296110774725094 0.0006156067109810445\n",
      "1 1.355640010209754 0.0006142969713181733\n",
      "101 1.3438594869803637 0.0006128187302418007\n",
      "201 1.3398014856502414 0.0006113511097561582\n",
      "301 1.2453488917089999 0.0006098939832926246\n",
      "401 1.74672898178801 0.0006084472263842588\n",
      "501 1.348103358541266 0.0006070107166211413\n",
      "601 1.2492765338683967 0.0006055843336068713\n",
      "701 1.568915182055207 0.0006041679589161831\n",
      "801 1.3617599749704823 0.0006027614760536461\n",
      "901 1.3296397840604186 0.0006013647704134199\n",
      "1001 1.506301498040557 0.0005999777292400283\n",
      "1101 1.1846984136500396 0.000598600241590126\n",
      "1201 1.1235853107646108 0.0005972321982952243\n",
      "1301 1.3506322290195385 0.0005958734919253515\n",
      "1401 1.5431637589354068 0.0005945240167536175\n",
      "1501 1.4227895765798166 0.0005931836687216574\n",
      "1601 1.2444980588334147 0.0005918523454059284\n",
      "1701 1.37204463215312 0.000590529945984835\n",
      "1801 1.3662666375166737 0.0005892163712066582\n",
      "1901 1.758998476434499 0.0005879115233582672\n",
      "2001 1.3996043455335894 0.0005866153062345879\n",
      "2101 1.409632071852684 0.0005853276251088103\n",
      "2201 1.3139934270293452 0.0005840483867033116\n",
      "2301 1.2863777373568155 0.0005827774991612753\n",
      "2401 1.1966209802776575 0.0005815148720189864\n",
      "2501 1.3174833165830933 0.0005802604161787846\n",
      "2601 1.406668136944063 0.0005790140438826557\n",
      "2701 1.31760111481708 0.0005777756686864456\n",
      "2801 1.22686495014932 0.0005765452054346768\n",
      "2901 1.4871160766715548 0.0005753225702359537\n",
      "3001 1.3321835576352896 0.0005741076804389384\n",
      "3101 1.349290698301047 0.0005729004546088824\n",
      "3201 1.0498975263908505 0.0005717008125046992\n",
      "3301 1.4295434548403136 0.0005705086750565621\n",
      "3401 1.3862976277887356 0.0005693239643440145\n",
      "3501 1.3612052928074263 0.0005681466035745775\n",
      "3601 1.3539716337691061 0.0005669765170628427\n",
      "3701 1.3053378225304186 0.0005658136302100359\n",
      "3801 1.2067344364186283 0.0005646578694840415\n",
      "3901 1.417662046442274 0.0005635091623998715\n",
      "4001 1.2578378450434684 0.0005623674375005725\n",
      "4101 1.2363171717152 0.0005612326243385544\n",
      "4201 1.3426340871083084 0.0005601046534573332\n",
      "4301 1.3097076122212457 0.000558983456373675\n",
      "4401 1.0131576862186193 0.0005578689655601316\n",
      "4501 1.4332989812392043 0.000556761114427959\n",
      "4601 1.4043821960221976 0.0005556598373104054\n",
      "4701 1.373746110650245 0.0005545650694463629\n",
      "4801 1.2657524709356949 0.0005534767469643717\n",
      "4901 1.1224889098666608 0.0005523948068669684\n",
      "5001 1.2615516305086203 0.000551319187015369\n",
      "5101 1.409785834257491 0.0005502498261144795\n",
      "5201 1.3791224808810512 0.0005491866636982242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5301 1.2408291140163783 0.0005481296401151859\n",
      "5401 1.3008261130889878 0.0005470786965145471\n",
      "5501 1.1700160388209042 0.0005460337748323287\n",
      "5601 1.2999350049067289 0.000544994817777915\n",
      "5701 1.3322585223941132 0.0005439617688208604\n",
      "5801 1.254337038320955 0.0005429345721779703\n",
      "5901 1.773689029644629 0.000541913172800649\n",
      "6001 1.3898115772462916 0.0005408975163625087\n",
      "6101 1.4735579792177305 0.0005398875492472326\n",
      "6201 1.05738219874911 0.000538883218536687\n",
      "6301 1.0802461032290012 0.0005378844719992749\n",
      "1 1.286231731530279 0.0005370101533168812\n",
      "101 1.2250633136718534 0.0005360217659787991\n",
      "201 1.239320948603563 0.0005350388161199592\n",
      "301 1.4140636462761904 0.0005340612540665886\n",
      "401 1.442663955502212 0.0005330890307779102\n",
      "501 1.4505203103472013 0.0005321220978358095\n",
      "601 1.2115196966333315 0.0005311604074347066\n",
      "701 1.2035035027656704 0.0005302039123716286\n",
      "801 1.3747974793659523 0.0005292525660364788\n",
      "901 1.36490419106849 0.0005283063224024965\n",
      "1001 1.1864821948111057 0.0005273651360169036\n",
      "1101 1.1623371304303873 0.000526428961991735\n",
      "1201 1.1043747729854658 0.0005254977559948457\n",
      "1301 1.6982813560443901 0.0005245714742410941\n",
      "1401 1.2719842366641387 0.0005236500734836944\n",
      "1501 1.2951120301149786 0.0005227335110057353\n",
      "1601 1.580276207998395 0.0005218217446118628\n",
      "1701 1.218743062199792 0.0005209147326201215\n",
      "1801 1.1479590674862266 0.0005200124338539494\n",
      "1901 1.2872504810075043 0.0005191148076343284\n",
      "2001 1.8993003838438653 0.0005182218137720798\n",
      "2101 1.2762204335303977 0.0005173334125603075\n",
      "2201 1.6183682525045242 0.0005164495647669814\n",
      "2301 1.2522982619411778 0.0005155702316276618\n",
      "2401 1.2925108795752749 0.0005146953748383575\n",
      "2501 1.340747339767404 0.0005138249565485178\n",
      "2601 1.340512964350637 0.0005129589393541545\n",
      "2701 1.1672844442073256 0.0005120972862910908\n",
      "2801 1.257948145037517 0.0005112399608283344\n",
      "2901 1.510728154462413 0.0005103869268615725\n",
      "3001 1.4130934766726568 0.0005095381487067851\n",
      "3101 1.2367545471934136 0.0005086935910939762\n",
      "3201 1.3846962348325178 0.0005078532191610173\n",
      "3301 1.2582954101526411 0.0005070169984476032\n",
      "3401 1.1545094328466803 0.0005061848948893172\n",
      "3501 1.295005505089648 0.0005053568748118022\n",
      "3601 1.3319955187034793 0.0005045329049250373\n",
      "3701 1.3548947679810226 0.000503712952317716\n",
      "3801 1.4635376840888057 0.0005028969844517252\n",
      "3901 1.6542128916307774 0.0005020849691567213\n",
      "4001 1.3512894048908493 0.0005012768746248036\n",
      "4101 1.397591198408918 0.0005004726694052806\n",
      "4201 1.3055676214280538 0.0004996723223995292\n",
      "4301 1.3375271083787084 0.000498875802855943\n",
      "4401 1.2366086341207847 0.0004980830803649704\n",
      "4501 1.2439679206581786 0.0004972941248542376\n",
      "4601 1.352382222772576 0.0004965089065837576\n",
      "4701 1.7570512742054234 0.0004957273961412208\n",
      "4801 1.232903058291413 0.0004949495644373684\n",
      "4901 1.015858386293985 0.0004941753827014446\n",
      "5001 1.381107110035373 0.000493404822476726\n",
      "5101 0.9564947709441185 0.0004926378556161293\n",
      "5201 1.228621664486127 0.0004918744542778926\n",
      "5301 1.182083563413471 0.0004911145909213302\n",
      "5401 1.2583643229590962 0.0004903582383026592\n",
      "5501 1.404046923678834 0.0004896053694708976\n",
      "5601 1.2389367091745953 0.0004888559577638302\n",
      "5701 1.119320425321348 0.00048810997680404295\n",
      "5801 1.586507015679672 0.0004873674004950231\n",
      "5901 1.112720330056618 0.0004866282030173253\n",
      "6001 1.3577893248293549 0.0004858923588248005\n",
      "6101 1.217524498468265 0.0004851598426408882\n",
      "6201 1.3229771983387764 0.0004844306294549693\n",
      "6301 1.5693217546272535 0.0004837046945187796\n",
      "1 1.1786362157727126 0.00048306134975017534\n",
      "101 1.28241519164294 0.00048234154403106603\n",
      "201 1.1411214591062162 0.00048162494648183897\n",
      "301 1.2352831599419005 0.0004809115333417623\n",
      "401 1.1032181181944907 0.00048020128109574806\n",
      "501 1.18390864826506 0.00047949416647109663\n",
      "601 1.2226583541632863 0.00047879016643429347\n",
      "701 1.0373018080717884 0.0004780892581878584\n",
      "801 1.2819566036341712 0.00047739141916724456\n",
      "901 1.1648676298791543 0.0004766966270377871\n",
      "1001 1.1654199322802015 0.00047600485969170105\n",
      "1101 1.2386636545270449 0.00047531609524512704\n",
      "1201 1.2253044219105504 0.0004746303120352227\n",
      "1301 1.375744077755371 0.0004739474886173019\n",
      "1401 1.1551300736318808 0.0004732676037620178\n",
      "1501 1.5255512128351256 0.00047259063645259034\n",
      "1601 1.255034319277911 0.00047191656588207824\n",
      "1701 1.1623500876303297 0.0004712453714506923\n",
      "1801 1.2958592986833537 0.00047057703276315175\n",
      "1901 1.1341320046922192 0.0004699115296260807\n",
      "2001 1.1937441515619867 0.0004692488420454462\n",
      "2101 1.7062073841661913 0.00046858895022403485\n",
      "2201 1.2566360468044877 0.00046793183455896863\n",
      "2301 1.2216275975806639 0.0004672774756392595\n",
      "2401 1.2636524712725077 0.0004666258542434008\n",
      "2501 1.2113699619076215 0.00046597695133699556\n",
      "2601 1.1559934263350442 0.00046533074807042176\n",
      "2701 1.256740387296304 0.0004646872257765319\n",
      "2801 1.3039579528664262 0.0004640463659683885\n",
      "2901 1.2651300196012016 0.0004634081503370334\n",
      "3001 1.2652980692801066 0.000462772560749291\n",
      "3101 1.1218284339411184 0.00046213957924560355\n",
      "3201 1.2543016897689085 0.0004615091880379007\n",
      "3301 1.2131407480192138 0.0004608813695074994\n",
      "3401 1.2994702684518415 0.0004602561062030357\n",
      "3501 1.2115506358095445 0.00045963338083842724\n",
      "3601 1.1760960748360958 0.00045901317629086643\n",
      "3701 1.0682971130590886 0.00045839547559884254\n",
      "3801 1.0764332090620883 0.00045778026196019347\n",
      "3901 1.1835216325707734 0.0004571675187301866\n",
      "4001 1.3529939632862806 0.00045655722941962654\n",
      "4101 1.3684578015236184 0.0004559493776929923\n",
      "4201 1.2233722301607486 0.0004553439473666001\n",
      "4301 1.2596116681525018 0.0004547409224067939\n",
      "4401 1.2757911044172943 0.00045414028692816196\n",
      "4501 1.2199301174841821 0.0004535420251917793\n",
      "4601 1.3471774608151463 0.0004529461216034753\n",
      "4701 1.475795219448628 0.0004523525607121267\n",
      "4801 1.1835241899825633 0.0004517613272079745\n",
      "4901 1.1791616377497576 0.0004511724059209659\n",
      "5001 1.3126113665202865 0.0004505857818191191\n",
      "5101 1.2516068609402282 0.0004500014400069121\n",
      "5201 1.178165558274486 0.0004494193657236937\n",
      "5301 1.6013869942435122 0.0004488395443421177\n",
      "5401 1.2677101592962572 0.00044826196136659916\n",
      "5501 1.1976667390699731 0.0004476866024317922\n",
      "5601 1.1990807302790927 0.00044711345330108884\n",
      "5701 1.1415361673789448 0.0004465424998651406\n",
      "5801 1.2389779405202717 0.0004459737281403985\n",
      "5901 1.1746156329172663 0.0004454071242676752\n",
      "6001 1.1718775559565984 0.0004448426745107265\n",
      "6101 1.1669323876558337 0.00044428036525485275\n",
      "6201 1.22836275130976 0.0004437201830055194\n",
      "6301 1.1068585112225264 0.000443162114386997\n",
      "1 1.1908240653865505 0.00044267275186678196\n",
      "101 1.156728027795907 0.0004421186210736662\n",
      "201 1.151486962888157 0.0004415665660409348\n",
      "301 1.1075408830074593 0.0004410165738412884\n",
      "401 1.1251853418070823 0.00044046863165985925\n",
      "501 1.224421168473782 0.0004399227267929559\n",
      "601 1.1097798637929372 0.00043937884664682695\n",
      "701 0.992531725903973 0.0004388369787364407\n",
      "801 1.2762772621936165 0.0004382971106842813\n",
      "901 1.154728337773122 0.00043775923021916087\n",
      "1001 0.9699444866273552 0.00043722332517504866\n",
      "1101 1.1039727496681735 0.0004366893834899152\n",
      "1201 1.2997219555545598 0.0004361573932045913\n",
      "1301 1.5713044246076606 0.00043562734246164385\n",
      "1401 1.1782071397465188 0.00043509921950426545\n",
      "1501 1.256332863289117 0.0004345730126751789\n",
      "1601 1.162631830346072 0.00043404871041555687\n",
      "1701 1.1123517343075946 0.00043352630126395546\n",
      "1801 1.0946980192093179 0.0004330057738552615\n",
      "1901 1.120711475959979 0.0004324871169196544\n",
      "2001 1.1385652619646862 0.0004319703192815812\n",
      "2101 1.0391206528292969 0.0004314553698587452\n",
      "2201 1.1468603002722375 0.00043094225766110786\n",
      "2301 1.1944863148819422 0.0004304309717899036\n",
      "2401 1.1445480604888871 0.00042992150143666746\n",
      "2501 1.160092411795631 0.0004294138358822756\n",
      "2601 0.905779943568632 0.00042890796449599795\n",
      "2701 1.2337692737637553 0.0004284038767345632\n",
      "2801 1.2654334787439439 0.00042790156214123586\n",
      "2901 1.2613030684588011 0.0004274010103449054\n",
      "3001 1.1566388571663992 0.0004269022110591865\n",
      "3101 1.1506170178181492 0.0004264051540815317\n",
      "3201 1.1042177192866802 0.00042590982929235444\n",
      "3301 1.268968387885252 0.00042541622665416415\n",
      "3401 1.1708880871301517 0.0004249243362107117\n",
      "3501 1.1094016103306785 0.00042443414808614573\n",
      "3601 1.3188527839665767 0.0004239456524841804\n",
      "3701 1.2144307589042 0.0004234588396872726\n",
      "3801 1.1827894128946355 0.0004229737000558104\n",
      "3901 0.9924444004427642 0.00042249022402731095\n",
      "4001 1.1228576390712988 0.0004220084021156294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101 1.1924936635477934 0.0004215282249101765\n",
      "4201 1.1275967326655518 0.0004210496830751471\n",
      "4301 1.0625419117277488 0.0004205727673487576\n",
      "4401 1.1389823842037003 0.00042009746854249313\n",
      "4501 1.339291847194545 0.00041962377754036395\n",
      "4601 1.0302886090357788 0.0004191516852981713\n",
      "4701 1.7122778899138211 0.00041868118284278167\n",
      "4801 1.2910672437865287 0.0004182122612714111\n",
      "4901 1.0494152382598259 0.00041774491175091685\n",
      "5001 1.1782474033534527 0.0004172791255170995\n",
      "5101 1.040663594380021 0.0004168148938740118\n",
      "5201 0.9901199785526842 0.00041635220819327733\n",
      "5301 1.5490817801910453 0.00041589105991341656\n",
      "5401 1.1346296942792833 0.00041543144053918197\n",
      "5501 1.223581779631786 0.00041497334164089994\n",
      "5601 0.958975835936144 0.0004145167548538224\n",
      "5701 1.238148811913561 0.0004140616718774844\n",
      "5801 1.1881117207813077 0.000413608084475071\n",
      "5901 1.1295715225860476 0.0004131559844727907\n",
      "6001 1.0610488005331717 0.000412705363759257\n",
      "6101 1.2371235750615597 0.00041225621428487707\n",
      "6201 1.1479767516138963 0.00041180852806124783\n",
      "6301 1.2059640220250003 0.0004113622971605593\n",
      "1 1.1765662879188312 0.0004109663692823915\n",
      "101 1.219779463717714 0.0004105228675028437\n",
      "201 0.972488499362953 0.00041008079847008953\n",
      "301 1.1617506150214467 0.0004096401544864815\n",
      "401 1.0883347367926035 0.0004092009279121472\n",
      "501 1.1085488446406089 0.00040876311116443343\n",
      "601 1.1023443718672752 0.0004083266967173559\n",
      "701 1.018611608800711 0.00040789167710105623\n",
      "801 1.1658196483003849 0.00040745804490126497\n",
      "901 1.2917855954219704 0.0004070257927587706\n",
      "1001 1.186474629881559 0.00040659491336889525\n",
      "1101 1.127356821874855 0.00040616539948097586\n",
      "1201 1.1975307842949405 0.00040573724389785204\n",
      "1301 1.1174790017685154 0.0004053104394753595\n",
      "1401 1.0863252188792103 0.0004048849791218294\n",
      "1501 1.0622602235816885 0.000404460855797593\n",
      "1601 1.1195327076129615 0.00040403806251449327\n",
      "1701 1.2447982146404684 0.00040361659233540054\n",
      "1801 1.2607627244724426 0.0004031964383737348\n",
      "1901 1.278331945562968 0.00040277759379299307\n",
      "2001 1.025594950420782 0.0004023600518062819\n",
      "2101 1.115274733179831 0.0004019438056758561\n",
      "2201 1.1167924739420414 0.0004015288487126612\n",
      "2301 1.0995151306560729 0.000401115174275883\n",
      "2401 1.1547567544039339 0.00040070277577250023\n",
      "2501 1.1590442548913416 0.00040029164665684384\n",
      "2601 1.1047132272506133 0.00039988178043016053\n",
      "2701 1.0620037270709872 0.00039947317064018093\n",
      "2801 1.0939110746548977 0.00039906581088069363\n",
      "2901 0.9693534299731255 0.0003986596947911227\n",
      "3001 1.2754340882529505 0.0003982548160561108\n",
      "3101 2.0011723663365046 0.0003978511684051071\n",
      "3201 1.1672507325711194 0.0003974487456119586\n",
      "3301 0.8547125565819442 0.00039704754149450736\n",
      "3401 1.0948779656609986 0.00039664754991419163\n",
      "3501 1.1662541554399013 0.0003962487647756509\n",
      "3601 1.242357063729287 0.00039585118002633614\n",
      "3701 1.142975198366912 0.000395454789656124\n",
      "3801 1.2055247909738682 0.0003950595876969351\n",
      "3901 1.2129587295930833 0.0003946655682223565\n",
      "4001 1.239052205113694 0.0003942727253472687\n",
      "4101 1.1285374546132516 0.0003938810532274764\n",
      "4201 1.3123125492420513 0.00039349054605934306\n",
      "4301 1.2088057449145708 0.00039310119807943006\n",
      "4401 1.0897887839237228 0.00039271300356413926\n",
      "4501 1.2131327866227366 0.00039232595682935973\n",
      "4601 0.9877158605959266 0.000391940052230118\n",
      "4701 1.2145014504967548 0.0003915552841602323\n",
      "4801 1.1513765653944574 0.0003911716470519708\n",
      "4901 1.1751896993955597 0.0003907891353757127\n",
      "5001 1.1771067604749987 0.0003904077436396139\n",
      "5101 1.3224336538114585 0.0003900274663892758\n",
      "5201 1.355893952131737 0.0003896482982074174\n",
      "5301 1.1996791153214872 0.0003892702337135512\n",
      "5401 1.206806231304654 0.0003888932675636631\n",
      "5501 1.280991047504358 0.0003885173944498942\n",
      "5601 0.9168080711970106 0.0003881426091002278\n",
      "5701 1.1924245768459514 0.0003877689062781782\n",
      "5801 1.1940782586170826 0.0003873962807824836\n",
      "5901 1.2784329915011767 0.0003870247274468023\n",
      "6001 1.1448089724872261 0.00038665424113941134\n",
      "6101 1.1181278676813236 0.0003862848167629092\n",
      "6201 1.2686003018752672 0.00038591644925392126\n",
      "6301 1.0795898175565526 0.000385549133582808\n",
      "1 1.199639980099164 0.00038523407955521927\n",
      "101 1.0967486363369972 0.00038486870703897236\n",
      "201 1.2039306252845563 0.00038450437215947677\n",
      "301 1.106695241353009 0.0003841410700146326\n",
      "401 1.0133273621067929 0.00038377879573470126\n",
      "501 1.1209047999582253 0.0003834175444820315\n",
      "601 1.073817516444251 0.00038305731145078797\n",
      "701 1.1002086726948619 0.00038269809186668256\n",
      "801 1.044247637852095 0.00038233988098670897\n",
      "901 1.1538543488713913 0.00038198267409887953\n",
      "1001 1.1638364950194955 0.00038162646652196454\n",
      "1101 1.1222136826545466 0.00038127125360523515\n",
      "1201 1.0936923912668135 0.0003809170307282081\n",
      "1301 1.0790816302178428 0.0003805637933003932\n",
      "1401 1.0973517978854943 0.0003802115367610436\n",
      "1501 1.3543376340385294 0.00037986025657890806\n",
      "1601 1.0638599513913505 0.0003795099482519871\n",
      "1701 1.095864930888638 0.0003791606073072896\n",
      "1801 1.3785420355270617 0.00037881222930059356\n",
      "1901 1.0656100183841772 0.0003784648098162084\n",
      "2001 1.083574770949781 0.0003781183444667399\n",
      "2101 1.7192173111798184 0.0003777728288928577\n",
      "2201 1.090653446619399 0.0003774282587630644\n",
      "2301 1.1122783308528597 0.00037708462977346826\n",
      "2401 0.95696423901245 0.0003767419376475568\n",
      "2501 1.2160079335735645 0.0003764001781359734\n",
      "2601 1.2086039673304185 0.00037605934701629616\n",
      "2701 1.0832101813866757 0.00037571944009281874\n",
      "2801 1.013074157119263 0.00037538045319633314\n",
      "2901 1.0823434699559584 0.00037504238218391556\n",
      "3001 1.1612105248786975 0.0003747052229387128\n",
      "3101 0.9126896761590615 0.0003743689713697328\n",
      "3201 1.3341417479550728 0.00037403362341163505\n",
      "3301 0.9600269035436213 0.0003736991750245252\n",
      "3401 1.1916928359714802 0.0003733656221937497\n",
      "3501 1.4976106537505984 0.0003730329609296942\n",
      "3601 1.1840611910447478 0.0003727011872675824\n",
      "3701 1.3150727476167958 0.0003723702972672783\n",
      "3801 1.221188339870423 0.00037204028701308904\n",
      "3901 1.2026138188084587 0.0003717111526135708\n",
      "4001 1.3793403076779214 0.00037138289020133557\n",
      "4101 1.0772470782976598 0.0003710554959328607\n",
      "4201 1.0168679640773917 0.0003707289659882998\n",
      "4301 1.1685322925950459 0.00037040329657129513\n",
      "4401 1.1224966624286026 0.00037007848390879306\n",
      "4501 1.0253048577578738 0.00036975452425085955\n",
      "4601 1.2101853350850433 0.00036943141387049916\n",
      "4701 1.050108958443161 0.0003691091490634741\n",
      "4801 1.2717001468117815 0.00036878772614812674\n",
      "4901 1.1231291381409392 0.00036846714146520227\n",
      "5001 1.1141781120968517 0.00036814739137767423\n",
      "5101 0.9994667179416865 0.00036782847227057074\n",
      "5201 1.3557415267750912 0.0003675103805508032\n",
      "5301 1.504937146051816 0.0003671931126469962\n",
      "5401 1.0444834220979828 0.00036687666500931896\n",
      "5501 1.0159150707913795 0.0003665610341093186\n",
      "5601 1.4691102042561397 0.00036624621643975515\n",
      "5701 1.2679062836105004 0.0003659322085144373\n",
      "5801 1.1070539963402553 0.0003656190068680607\n",
      "5901 1.2043958652066067 0.0003653066080560474\n",
      "6001 1.1217296464601532 0.00036499500865438625\n",
      "6101 1.2132740695233224 0.00036468420525947586\n",
      "6201 1.452793362134571 0.00036437419448796804\n",
      "6301 1.0731251265387982 0.00036406497297661317\n",
      "1 1.1998733360724145 0.00036379350826718935\n",
      "101 1.1498671763110906 0.0003634857615296514\n",
      "201 1.1022596344992053 0.00036317879447701637\n",
      "301 1.0011312331771478 0.00036287260382257964\n",
      "401 1.0373523531015962 0.0003625671862990008\n",
      "501 1.0644397677097004 0.000362262538658157\n",
      "601 1.1183270415349398 0.000361958657670998\n",
      "701 0.9439565273642074 0.00036165554012740277\n",
      "801 1.0483181119780056 0.0003613531828360362\n",
      "901 1.10791165966657 0.00036105158262420917\n",
      "1001 0.9895736404578201 0.00036075073633773743\n",
      "1101 1.0942751504917396 0.00036045064084080426\n",
      "1201 1.0274720180314034 0.0003601512930158222\n",
      "1301 1.049829078532639 0.0003598526897632977\n",
      "1401 1.0599873741302872 0.00035955482800169595\n",
      "1501 1.1110646773013286 0.00035925770466730756\n",
      "1601 1.1195740707335062 0.0003589613167141159\n",
      "1701 1.1175601889844984 0.0003586656611136664\n",
      "1801 1.27650167158572 0.00035837073485493607\n",
      "1901 1.1153064398095012 0.000358076534944205\n",
      "2001 1.4756392514391337 0.000357783058404929\n",
      "2101 0.8967400579713285 0.0003574903022776121\n",
      "2201 1.0554919667192735 0.0003571982636196827\n",
      "2301 1.1484477042686194 0.000356906939505368\n",
      "2401 1.0967925000586547 0.00035661632702557175\n",
      "2501 1.275310180048109 0.0003563264232877516\n",
      "2601 1.084061863599345 0.00035603722541579873\n",
      "2701 1.076280384673737 0.00035574873054991784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801 1.200010517553892 0.0003554609358465082\n",
      "2901 1.059172057226533 0.000355173838478046\n",
      "3001 1.0261963941156864 0.000354887435632968\n",
      "3101 0.9737269952893257 0.0003546017245155551\n",
      "3201 1.109491402952699 0.0003543167023458187\n",
      "3301 1.0379895093501545 0.0003540323663593864\n",
      "3401 1.0210047286818735 0.00035374871380738974\n",
      "3501 1.3062616163679195 0.0003534657419563522\n",
      "3601 1.1297821700572968 0.00035318344808807914\n",
      "3701 1.095454223890556 0.0003529018294995477\n",
      "3801 1.0263875699602067 0.00035262088350279793\n",
      "3901 1.0200049103004858 0.00035234060742482575\n",
      "4001 1.139240143907955 0.00035206099860747537\n",
      "4101 1.185085133272878 0.00035178205440733397\n",
      "4201 1.0887831579057092 0.0003515037721956263\n",
      "4301 1.7533796445081862 0.000351226149358111\n",
      "4401 1.1149956742010545 0.00035094918329497705\n",
      "4501 1.1544227619015146 0.0003506728714207421\n",
      "4601 1.1121961249154992 0.00035039721116415036\n",
      "4701 1.0929120010696352 0.0003501221999680728\n",
      "4801 1.118996370700188 0.000349847835289407\n",
      "4901 0.9860638748505153 0.00034957411459897886\n",
      "5001 1.004449057742022 0.0003493010353814441\n",
      "5101 1.2927988782757893 0.00034902859513519165\n",
      "5201 0.98420900356723 0.0003487567913722472\n",
      "5301 1.1210692654858576 0.000348485621618178\n",
      "5401 1.163566045666812 0.00034821508341199766\n",
      "5501 1.17701395630138 0.0003479451743060731\n",
      "5601 1.0291424575298151 0.00034767589186603104\n",
      "5701 1.2543358486509533 0.0003474072336706659\n",
      "5801 1.2299754508348997 0.00034713919731184855\n",
      "5901 1.1936145080917413 0.0003468717803944353\n",
      "6001 0.9138605990447104 0.00034660498053617827\n",
      "6101 1.1037570714397589 0.00034633879536763624\n",
      "6201 1.04157008238235 0.00034607322253208626\n",
      "6301 1.1919174637878314 0.0003458082596854357\n",
      "1 0.9797578346915543 0.00034557559511139293\n",
      "101 0.9891712895187084 0.00034531177274179953\n",
      "201 1.1815550197497942 0.00034504855367990236\n",
      "301 1.1358435613219626 0.0003447859356297997\n",
      "401 1.0717519058380276 0.000344523916307803\n",
      "501 1.172375235328218 0.00034426249344235384\n",
      "601 1.0603420100815129 0.00034400166477394084\n",
      "701 1.0992282917286502 0.000343741428055018\n",
      "801 1.04559408465866 0.0003434817810499231\n",
      "901 1.0248865495998416 0.0003432227215347973\n",
      "1001 1.0598302248690743 0.0003429642472975047\n",
      "1101 1.0938185814011376 0.0003427063561375535\n",
      "1201 1.291374852447916 0.000342449045866017\n",
      "1301 1.0285806620959193 0.0003421923143054557\n",
      "1401 1.17992360109929 0.0003419361592898398\n",
      "1501 0.9615428688703105 0.0003416805786644727\n",
      "1601 1.2486475716141285 0.0003414255702859144\n",
      "1701 0.9794061238644645 0.0003411711320219065\n",
      "1801 1.1059001302346587 0.00034091726175129706\n",
      "1901 0.9131126408465207 0.00034066395736396637\n",
      "2001 0.9930663524428383 0.0003404112167607534\n",
      "2101 1.0812218267819844 0.0003401590378533823\n",
      "2201 1.0715046060213353 0.0003399074185643907\n",
      "2301 1.1185214965953492 0.00033965635682705713\n",
      "2401 1.05721441534115 0.00033940585058533\n",
      "2501 1.0936700437378022 0.00033915589779375693\n",
      "2601 1.07034515045234 0.00033890649641741454\n",
      "2701 1.0813248989579733 0.00033865764443183875\n",
      "2801 1.0510470166627783 0.0003384093398229561\n",
      "2901 0.9623011860530823 0.0003381615805870148\n",
      "3001 1.1494725269731134 0.00033791436473051725\n",
      "3101 1.2335324875239166 0.0003376676902701525\n",
      "3201 1.0398004396120086 0.00033742155523272933\n",
      "3301 1.0589452146668918 0.0003371759576551101\n",
      "3401 1.084106142167002 0.00033693089558414497\n",
      "3501 1.0406659920408856 0.0003366863670766065\n",
      "3601 0.9325393754988909 0.00033644237019912526\n",
      "3701 1.0507557194505353 0.0003361989030281253\n",
      "3801 0.945562198292464 0.0003359559636497606\n",
      "3901 1.0489263081690297 0.0003357135501598519\n",
      "4001 1.0528855019947514 0.00033547166066382383\n",
      "4101 1.1952165458351374 0.0003352302932766432\n",
      "4201 1.0958911241032183 0.00033498944612275674\n",
      "4301 1.077680416405201 0.0003347491173360301\n",
      "4401 1.2972540742484853 0.0003345093050596873\n",
      "4501 1.039087069220841 0.0003342700074462501\n",
      "4601 0.9597453814931214 0.00033403122265747876\n",
      "4701 1.1728105103829876 0.00033379294886431207\n",
      "4801 1.1258336059836438 0.0003335551842468092\n",
      "4901 1.0011352995352354 0.0003333179269940906\n",
      "5001 0.9672558718448272 0.00033308117530428074\n",
      "5101 1.0522874586749822 0.0003328449273844502\n",
      "5201 1.0063609674107283 0.0003326091814505589\n",
      "5301 1.0480196221014921 0.00033237393572739917\n",
      "5401 1.0445173801199417 0.00033213918844854004\n",
      "5501 1.417743748796056 0.00033190493785627127\n",
      "5601 1.0902981872641249 0.000331671182201548\n",
      "5701 1.0301871165866032 0.00033143791974393625\n",
      "5801 1.4090074983541854 0.00033120514875155805\n",
      "5901 1.1904211936052889 0.0003309728675010378\n",
      "6001 1.1052953382313717 0.0003307410742774485\n",
      "6101 1.133972127106972 0.00033050976737425853\n",
      "6201 1.4820798086614104 0.00033027894509327907\n",
      "6301 1.1476092239608988 0.00033004860574461153\n",
      "1 1.0870586273958907 0.00032984630526377065\n",
      "101 1.00110832543578 0.00032961686928176145\n",
      "201 0.9876259700540686 0.0003293879114110055\n",
      "301 1.2413200094233616 0.0003291594299932822\n",
      "401 0.9424758276436478 0.00032893142337841173\n",
      "501 1.0926933737646323 0.00032870388992420444\n",
      "601 1.0706572374765528 0.0003284768279964114\n",
      "701 1.0879125816572923 0.00032825023596867546\n",
      "801 1.262531905740616 0.0003280241122224816\n",
      "901 1.050877535046311 0.0003277984551471088\n",
      "1001 1.135452825037646 0.0003275732631395822\n",
      "1101 1.0099836179433623 0.0003273485346046242\n",
      "1201 1.1641883124248125 0.0003271242679546084\n",
      "1301 1.0249766572378576 0.00032690046160951133\n",
      "1401 1.216345368164184 0.0003266771139968662\n",
      "1501 1.4009095890432945 0.00032645422355171653\n",
      "1601 0.9214687866624445 0.00032623178871657\n",
      "1701 1.050587208737852 0.0003260098079413526\n",
      "1801 1.0106142781150993 0.0003257882796833635\n",
      "1901 0.9388233295176178 0.00032556720240723\n",
      "2001 1.1458081254386343 0.0003253465745848626\n",
      "2101 1.0818304931126477 0.00032512639469541087\n",
      "2201 0.8635702040046453 0.0003249066612252194\n",
      "2301 1.0180434776411857 0.00032468737266778394\n",
      "2401 1.0467977939988486 0.0003244685275237081\n",
      "2501 1.083000476603047 0.0003242501243006605\n",
      "2601 1.1069668279960752 0.00032403216151333166\n",
      "2701 1.086160118225962 0.00032381463768339173\n",
      "2801 1.0924749624973629 0.0003235975513394485\n",
      "2901 1.009744831302669 0.00032338090101700554\n",
      "3001 0.9085385013604537 0.0003231646852584205\n",
      "3101 1.1706041378201917 0.00032294890261286426\n",
      "3201 1.032271361502353 0.00032273355163627964\n",
      "3301 1.2584509218577296 0.00032251863089134133\n",
      "3401 1.2436874122358859 0.000322304138947415\n",
      "3501 1.0451832013077365 0.0003220900743805179\n",
      "3601 1.0900762653473066 0.00032187643577327854\n",
      "3701 1.1192542002827395 0.00032166322171489793\n",
      "3801 1.00253647408681 0.0003214504308011099\n",
      "3901 1.2160937447333708 0.0003212380616341424\n",
      "4001 1.0416435159859248 0.0003210261128226793\n",
      "4101 1.3598752447869629 0.00032081458298182156\n",
      "4201 1.0555532689650136 0.0003206034707330495\n",
      "4301 1.1295962483854964 0.00032039277470418526\n",
      "4401 0.9410244208120275 0.0003201824935293548\n",
      "4501 0.8939700378105044 0.00031997262584895135\n",
      "4601 0.908640876179561 0.000319763170309598\n",
      "4701 1.128680162204546 0.0003195541255641112\n",
      "4801 1.0497526655672118 0.00031934549027146444\n",
      "4901 1.0289937005145475 0.0003191372630967521\n",
      "5001 0.9918764412868768 0.0003189294427111535\n",
      "5101 1.2255524442298338 0.0003187220277918973\n",
      "5201 1.3292681298672733 0.0003185150170222263\n",
      "5301 0.878005885053426 0.00031830840909136197\n",
      "5401 1.0740752452165907 0.00031810220269447\n",
      "5501 1.0994656120092259 0.00031789639653262544\n",
      "5601 1.159670107124839 0.0003176909893127784\n",
      "5701 0.8859041188843548 0.0003174859797477199\n",
      "5801 1.084522244927939 0.00031728136655604814\n",
      "5901 1.4824702723776682 0.0003170771484621346\n",
      "6001 1.230977819112013 0.0003168733241960908\n",
      "6101 1.0119965468620649 0.00031666989249373517\n",
      "6201 1.1002646164814678 0.00031646685209656003\n",
      "6301 1.1440792203939054 0.00031626420175169897\n",
      "1 1.0551144047021808 0.0003160882122689669\n",
      "101 1.0408390048833098 0.00031588628797931984\n",
      "201 1.0153360446565785 0.0003156847501772966\n",
      "301 1.01748421555385 0.0003154835976315582\n",
      "401 1.1267781729111448 0.0003152828291162512\n",
      "501 1.0327468327741371 0.0003150824434109757\n",
      "601 0.9960314880299848 0.0003148824393007546\n",
      "701 1.0631007702104398 0.00031468281557600267\n",
      "801 1.0372768385277595 0.00031448357103249544\n",
      "901 1.031023440795252 0.0003142847044713392\n",
      "1001 0.9323838343843818 0.0003140862146989404\n",
      "1101 0.850510573014617 0.0003138881005269756\n",
      "1201 1.0943628003296908 0.0003136903607723615\n",
      "1301 1.0844742289336864 0.00031349299425722566\n",
      "1401 1.4024500491796061 0.00031329599980887637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501 1.1463393379817717 0.00031309937625977405\n",
      "1601 1.0650637014914537 0.0003129031224475018\n",
      "1701 1.1410465109511279 0.00031270723721473664\n",
      "1801 1.0148204645956866 0.0003125117194092209\n",
      "1901 0.9743651752360165 0.0003123165678837336\n",
      "2001 0.990701739974611 0.00031212178149606226\n",
      "2101 1.1128740338463103 0.00031192735910897496\n",
      "2201 1.5508626039809315 0.0003117332995901923\n",
      "2301 1.01486110695987 0.00031153960181235955\n",
      "2401 0.9611194784665713 0.0003113462646530196\n",
      "2501 1.0847897573257796 0.0003111532869945851\n",
      "2601 1.2803321699175285 0.00031096066772431187\n",
      "2701 1.0769891024538083 0.0003107684057342714\n",
      "2801 1.0039243546780199 0.00031057649992132457\n",
      "2901 1.0824949400266632 0.00031038494918709473\n",
      "3001 0.9644128995714709 0.00031019375243794144\n",
      "3101 0.9868561172188492 0.00031000290858493437\n",
      "3201 1.0903575613629073 0.00030981241654382685\n",
      "3301 1.2633273452374851 0.0003096222752350304\n",
      "3401 1.0088038056419464 0.000309432483583589\n",
      "3501 1.1047235757578164 0.0003092430405191533\n",
      "3601 1.163446888080216 0.00030905394497595545\n",
      "3701 1.018205283649877 0.00030886519589278384\n",
      "3801 1.0408570388099179 0.00030867679221295824\n",
      "3901 1.0657447287230752 0.00030848873288430483\n",
      "4001 0.930832964291767 0.0003083010168591314\n",
      "4101 1.4587874389944773 0.00030811364309420327\n",
      "4201 1.1227497690124437 0.0003079266105507184\n",
      "4301 1.1970081577601377 0.0003077399181942835\n",
      "4401 1.1083186157047749 0.00030755356499488986\n",
      "4501 1.0473160178516991 0.00030736754992688985\n",
      "4601 1.0928417469840497 0.0003071818719689727\n",
      "4701 1.0073067757184617 0.00030699653010414117\n",
      "4801 1.1523846584532293 0.00030681152331968824\n",
      "4901 1.1988015054084826 0.0003066268506071739\n",
      "5001 1.036811558995396 0.00030644251096240176\n",
      "5101 1.0675939484208357 0.0003062585033853964\n",
      "5201 1.0752534797684348 0.00030607482688038056\n",
      "5301 1.1183025861246279 0.0003058914804557523\n",
      "5401 1.0365500289481133 0.0003057084631240629\n",
      "5501 1.129350705537945 0.00030552577390199393\n",
      "5601 1.06671357084997 0.0003053434118103358\n",
      "5701 1.0859052878222428 0.0003051613758739652\n",
      "5801 1.0723684425465763 0.00030497966512182316\n",
      "5901 1.0603833084605867 0.0003047982785868937\n",
      "6001 1.0581669194652932 0.0003046172153061818\n",
      "6101 1.2675022858026068 0.0003044364743206923\n",
      "6201 1.0536423055746127 0.0003042560546754083\n",
      "6301 1.2465427681345318 0.00030407595541926996\n",
      "1 0.8441335130482912 0.00030391413923858634\n",
      "101 0.9350836968515068 0.00030373464611573535\n",
      "201 1.0421846130630001 0.0003035554706461405\n",
      "301 1.008769184758421 0.0003033766118939749\n",
      "401 0.9787611065548845 0.000303198068927267\n",
      "501 1.0469113910803571 0.00030301984081788013\n",
      "601 1.0306272330635693 0.00030284192664149214\n",
      "701 0.8391264111269265 0.00030266432547757535\n",
      "801 0.9852646276758605 0.00030248703640937665\n",
      "901 0.9640902730752714 0.00030231005852389745\n",
      "1001 1.1280414578068303 0.00030213339091187405\n",
      "1101 0.9939612101297826 0.0003019570326677579\n",
      "1201 1.0867023059399799 0.00030178098288969626\n",
      "1301 1.0411206257122103 0.00030160524067951265\n",
      "1401 1.0711584523378406 0.0003014298051426879\n",
      "1501 1.0796883448783774 0.0003012546753883405\n",
      "1601 1.027666941517964 0.00030107985052920836\n",
      "1701 1.082986782770604 0.00030090532968162913\n",
      "1801 1.1074479344606516 0.0003007311119655219\n",
      "1901 1.1305997944582487 0.0003005571965043686\n",
      "2001 1.345339710366943 0.0003003835824251953\n",
      "2101 1.001590578132891 0.00030021026885855383\n",
      "2201 1.0054450589232147 0.0003000372549385033\n",
      "2301 1.0041432639409322 0.00029986453980259265\n",
      "2401 1.0640304164699046 0.00029969212259184163\n",
      "2501 1.1201181028736755 0.0002995200024507235\n",
      "2601 0.8765224074013531 0.00029934817852714696\n",
      "2701 1.0152945614827331 0.0002991766499724386\n",
      "2801 1.2956223709957158 0.0002990054159413251\n",
      "2901 1.097986907014274 0.0002988344755919157\n",
      "3001 1.2291080165232415 0.00029866382808568526\n",
      "3101 1.1140619127836544 0.0002984934725874564\n",
      "3201 1.0722678579004423 0.0002983234082653825\n",
      "3301 1.03946516571159 0.0002981536342909311\n",
      "3401 0.9876702070032479 0.0002979841498388662\n",
      "3501 0.8540887358831242 0.00029781495408723205\n",
      "3601 0.9894529741141014 0.00029764604621733594\n",
      "3701 1.0710785342380404 0.000297477425413732\n",
      "3801 1.2710673977526312 0.00029730909086420423\n",
      "3901 1.0929949116252828 0.0002971410417597504\n",
      "4001 1.1222996068827342 0.0002969732772945655\n",
      "4101 1.164539644116303 0.00029680579666602566\n",
      "4201 1.033734397671651 0.000296638599074672\n",
      "4301 1.093015514779836 0.0002964716837241944\n",
      "4401 1.0481613585725427 0.0002963050498214161\n",
      "4501 1.0937337041832507 0.00029613869657627706\n",
      "4601 1.1815662420112858 0.000295972623201819\n",
      "4701 1.1428916620570817 0.0002958068289141693\n",
      "4801 1.1009264337189961 0.0002956413129325257\n",
      "4901 0.8777621657354757 0.00029547607447914055\n",
      "5001 1.0156730558082927 0.00029531111277930595\n",
      "5101 0.9942513670539483 0.00029514642706133804\n",
      "5201 1.1302866424011881 0.00029498201655656206\n",
      "5301 1.0087051462905947 0.0002948178804992971\n",
      "5401 1.0660143050336046 0.0002946540181268415\n",
      "5501 1.0107977577135898 0.00029449042867945755\n",
      "5601 1.0777363086963305 0.0002943271114003569\n",
      "5701 0.856828257907182 0.00029416406553568584\n",
      "5801 1.1287360956775956 0.0002940012903345107\n",
      "5901 1.366358119645156 0.00029383878504880313\n",
      "6001 1.0964845723065082 0.00029367654893342604\n",
      "6101 1.34646458978159 0.00029351458124611887\n",
      "6201 1.2197050878312439 0.0002933528812474836\n",
      "6301 1.0830936049751472 0.0002931914482009704\n",
      "1 1.2537849597129025 0.00029304477550482497\n",
      "101 0.9655292083625682 0.00029288385030021\n",
      "201 1.1372923650196753 0.0002927231899204302\n",
      "301 0.9451354363700375 0.0002925627936399378\n",
      "401 0.9661671929707154 0.00029240266073596516\n",
      "501 1.0162687979172915 0.0002922427904885108\n",
      "601 0.9551542007829994 0.0002920831821803257\n",
      "701 0.9841917234880384 0.0002919238350969\n",
      "801 1.061543255826109 0.00029176474852644945\n",
      "901 0.985908080736408 0.0002916059217599022\n",
      "1001 1.0337736615701942 0.0002914473540908853\n",
      "1101 0.9899544979416532 0.0002912890448157118\n",
      "1201 1.1052642236463726 0.00029113099323336726\n",
      "1301 0.9609193275682628 0.00029097319864549706\n",
      "1401 1.0143777604680508 0.0002908156603563932\n",
      "1501 1.0131031578639522 0.0002906583776729816\n",
      "1601 1.0399196342332289 0.00029050134990480915\n",
      "1701 1.3567781529854983 0.00029034457636403104\n",
      "1801 1.1145936762022757 0.0002901880563653981\n",
      "1901 1.0984270876506343 0.0002900317892262443\n",
      "2001 1.0226630433771788 0.00028987577426647405\n",
      "2101 1.1856945900362916 0.0002897200108085499\n",
      "2201 1.125245438015554 0.00028956449817748025\n",
      "2301 1.126162831991678 0.00028940923570080693\n",
      "2401 0.971063018507266 0.00028925422270859307\n",
      "2501 0.8773902256507427 0.00028909945853341086\n",
      "2601 0.8763325407635421 0.0002889449425103295\n",
      "2701 1.1415061227562546 0.0002887906739769035\n",
      "2801 1.052460735765635 0.0002886366522731603\n",
      "2901 1.5205009760629764 0.00028848287674158846\n",
      "3001 0.9414957111439435 0.0002883293467271265\n",
      "3101 0.9435001520323567 0.0002881760615771502\n",
      "3201 1.1403545759221743 0.0002880230206414618\n",
      "3301 1.053262686386006 0.00028787022327227786\n",
      "3401 1.0832857484929264 0.000287717668824218\n",
      "3501 1.1019753235159442 0.00028756535665429354\n",
      "3601 1.055358653771691 0.0002874132861218958\n",
      "3701 1.0278627741499804 0.00028726145658878504\n",
      "3801 1.0083879251906183 0.0002871098674190792\n",
      "3901 1.0555589499126654 0.0002869585179792425\n",
      "4001 1.0509156602493022 0.00028680740763807453\n",
      "4101 1.0385481148259714 0.0002866565357666993\n",
      "4201 1.0417402729653986 0.0002865059017385537\n",
      "4301 1.082753369351849 0.0002863555049293774\n",
      "4401 1.0459589868987678 0.0002862053447172013\n",
      "4501 1.0592919351911405 0.00028605542048233684\n",
      "4601 1.1034397517796606 0.0002859057316073656\n",
      "4701 1.1311876591207692 0.00028575627747712837\n",
      "4801 1.070465801298269 0.00028560705747871445\n",
      "4901 1.199700104945805 0.00028545807100145134\n",
      "5001 1.2379299406893551 0.00028530931743689397\n",
      "5101 1.1045849512156565 0.00028516079617881457\n",
      "5201 0.9958119990806154 0.000285012506623192\n",
      "5301 1.620139996672151 0.00028486444816820157\n",
      "5401 1.0613090786646353 0.0002847166202142048\n",
      "5501 1.175794189737644 0.0002845690221637393\n",
      "5601 1.1241089710965753 0.00028442165342150834\n",
      "5701 1.160597581154434 0.000284274513394371\n",
      "5801 1.0310369414401066 0.0002841276014913322\n",
      "5901 0.9960121748881647 0.0002839809171235324\n",
      "6001 0.9698299318188219 0.00028383445970423817\n",
      "6101 1.1415085992775857 0.0002836882286488319\n",
      "6201 1.0641657677479088 0.0002835422233748022\n",
      "6301 1.0828722650112468 0.00028339644330173413\n"
     ]
    }
   ],
   "source": [
    "#weight = torch.ones(len(TGT.vocab))\n",
    "#weight[pad_idx] = 0\n",
    "#criterion = nn.NLLLoss(size_average=False, weight=weight.cuda())\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 10.825187489390373 6.987712429686844e-07\n",
    "101 9.447168171405792 3.56373333914029e-05\n",
    "201 7.142856806516647 7.057589553983712e-05\n",
    "301 6.237934365868568 0.00010551445768827134\n",
    "401 5.762486848048866 0.00014045301983670557\n",
    "501 5.415792358107865 0.00017539158198513977\n",
    "601 5.081815680023283 0.000210330144133574\n",
    "701 4.788327748770826 0.00024526870628200823\n",
    "801 4.381739928154275 0.0002802072684304424\n",
    "901 4.55433791608084 0.00031514583057887664\n",
    "1001 4.911875109748507 0.0003500843927273108\n",
    "1101 4.0579032292589545 0.0003850229548757451\n",
    "1201 4.2276234351193125 0.0004199615170241793\n",
    "1301 3.932735869428143 0.00045490007917261356\n",
    "1401 3.8179439397063106 0.0004898386413210477\n",
    "1501 3.3608515430241823 0.000524777203469482\n",
    "1601 3.832796103321016 0.0005597157656179162\n",
    "1701 2.907085266895592 0.0005946543277663504\n",
    "1801 3.5280659823838505 0.0006295928899147847\n",
    "1901 2.895841649500653 0.0006645314520632189\n",
    "2001 3.273784235585481 0.000699470014211653\n",
    "2101 3.181488689899197 0.0007344085763600873\n",
    "2201 3.4151616653980454 0.0007693471385085215\n",
    "2301 3.4343731447588652 0.0008042857006569557\n",
    "2401 3.0505455391539726 0.0008392242628053899\n",
    "2501 2.8089329147478566 0.0008741628249538242\n",
    "2601 2.7827929875456903 0.0009091013871022583\n",
    "2701 2.4428516102489084 0.0009440399492506926\n",
    "2801 2.4015486147254705 0.0009789785113991267\n",
    "2901 2.3568112018401735 0.001013917073547561\n",
    "3001 2.6349758653668687 0.0010488556356959952\n",
    "3101 2.5981983028614195 0.0010837941978444295\n",
    "3201 2.666826274838968 0.0011187327599928637\n",
    "3301 3.0092043554177508 0.0011536713221412978\n",
    "3401 2.4580375660589198 0.0011886098842897321\n",
    "3501 2.586465588421561 0.0012235484464381662\n",
    "3601 2.5663993963389657 0.0012584870085866006\n",
    "3701 2.9430236657499336 0.0012934255707350347\n",
    "3801 2.464644919440616 0.001328364132883469\n",
    "3901 2.7124062888276512 0.0013633026950319032\n",
    "4001 2.646443709731102 0.0013971932312809247\n",
    "4101 2.7294750874862075 0.001380057517579748\n",
    "4201 2.1295202329056337 0.0013635372009002666\n",
    "4301 2.596563663915731 0.001347596306985731\n",
    "4401 2.1265982036820787 0.0013322017384983986\n",
    "4501 2.3880532500334084 0.0013173229858148\n",
    "4601 2.6129120760888327 0.0013029318725783852\n",
    "4701 2.2873719420749694 0.001289002331178292\n",
    "4801 2.4949760700110346 0.0012755102040816328\n",
    "4901 2.496607314562425 0.001262433067573089\n",
    "5001 2.1889712483389303 0.0012497500749750088\n",
    "5101 1.8677761815488338 0.0012374418168536253\n",
    "5201 2.2992054556962103 0.0012254901960784316\n",
    "5301 2.664361578106707 0.0012138783159049418\n",
    "5401 2.705850490485318 0.0012025903795063202\n",
    "5501 2.581445264921058 0.0011916115995949978\n",
    "5601 2.2480602325085783 0.0011809281169581616\n",
    "5701 1.9289666265249252 0.0011705269268863989\n",
    "5801 2.4863578918157145 0.0011603958126073107\n",
    "5901 2.632946971571073 0.0011505232849492607\n",
    "6001 2.496141305891797 0.0011408985275576757\n",
    "6101 2.6422974687084206 0.0011315113470699342\n",
    "6201 2.448802186456305 0.0011223521277270118"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
